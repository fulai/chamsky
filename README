运行source/bin/release下的exe，debug下的效率可能比较低

“exe”里面有两个文件，large里面的放是1000文本的分类程序，small里面放的是100个文本的分类程序

过程中界面可能会发生假死，请不要强行关闭！

【关于文本分类】*********************************************************************

《中文文本信息处理》这门课是我在大学中接触到的第一门“有用”的课程，每天我都可以感受到它在我的生活中发挥着作用，有谁没用过google、输入法和拼写检查呢？当然并不是说其他课程就“没用”了，像操作系统和组成原理这样的课，除非真的倒腾个电脑出来、写个操作系统出来，否则很难与实践扯上关系。加上这些课都是考研的科目，所以难免让这些课蒙上形式化的阴影，但《中文》不一样，至少我在网上搜索相关东西的时候找不到那种习题之类的东西，而且每个知识点都有对应的应用点。

通过这门课和一些实践，至少让我学会了两样东西。第一是重新学习的能力。我在上这课的时的最大的感觉就是数学知识完全不够用，很多时候只是背了公式，而完全不懂蕴含在背后的内涵，比如平凡而又神奇的贝叶斯定理。为了写好程序，我将概率论中的很多知识又学了一遍，比如假设检验的几个模型，当时学的时候那个章节根本没教，难怪这学期老师在课上讲卡方分布的时候我完全听不懂。于此同时我拜读了吴军的《数学之美》，让我重新地审视了数学这门“重要的”学科，并且认识到了它在信息检索、数据挖掘或者说“中文文本信息处理”上的作用。我还阅读了一些自然语言处理方面的书（因为时间有限，未能深入学习），了解到了以乔姆斯基和贾里尼克为代表的经验主义和统计学家的两派之争，我也终于明白李开复博士是干嘛的了。

第二是查资料的能力，这里的查资料并不是说在维基百科上敲入一个关键词然后得到一个答案，而是通过阅读他人的文章（博客或者程序文档）而学习，网上有很多NLP爱好者的言论很值得一看，比如一开始我一直很糊涂特征抽取和特征权重的区别，直到看到一个帖子才豁然开朗，等我搞明白之后自己也写了两篇文章出来。

虽然我选的是第一个作业――文本分类，但因为久仰在weka在大名，也学了一下怎么用，发现文本分类只是数据挖掘这片汪洋大海上的一叶扁舟，自己看到的更只是冰山一角。

【关于程序】*******************************************************************************

1.程序虽然写得很烂，但是把一个文本分类的基本框架搭了出来，在每个功能上我尽可能把它们分层实现，调试起来可能方便一些。

2.程序有很多“毛病”，程序是单线程，所以在显示进度的时候可能会假死。加上很多都是现学现卖，毫无设计可言，算是一大失败。最应该改进的地方是，每个层次的中间结果最好序列化一下保存，定义一个类似arff格式这样的，这样就不必每次训练以前都转化了。

3.更多关于程序更详细的说明，我已写在“文本分类实验报告”中。

【关于语料库】*****************************************************************************

1.语料库用的是搜狗实验室的那个，选了1000篇出来，其中700篇做训练，300篇做测试，用来测试“分类器”。这1000个文本放在了一个叫bar的文件中。

2.另外有一个foo的文件夹，里有100篇文档，用来调试程序。

【参考文献】********************************************************************************

1.中文文本信息处理的原理与应用（教材）

2.统计自然语言处理（宗成庆）

3.weka入门教程（IDMer）

4.统计自然语言处理基础（紫皮）

5.文本分类入门 http://www.blogjava.net/zhenandaci/category/31868.html?Show=All

6.谷歌黑板报（吴军）

7.基于统计分词的中文文本分类系统（吴雅娟）
